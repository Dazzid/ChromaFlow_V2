{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import tokenizers\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "dataset = datasets.load_from_disk('./hf_dataset')\n",
    "tokenized_datasets = datasets.load_from_disk('./hf_dataset_tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0\n",
    "max_length = 1024\n",
    "tokens = np.load('../data/formatted/tokens.npy', allow_pickle=True)\n",
    "\n",
    "if dataset == None:\n",
    "    train = np.load('../data/shuffled/dataset_train.npy', allow_pickle=True)\n",
    "    test = np.load('../data/shuffled/dataset_test.npy', allow_pickle=True)\n",
    "    midi_train = np.load('../data/shuffled/midi_train.npy', allow_pickle=True)\n",
    "    midi_test = np.load('../data/shuffled/midi_test.npy', allow_pickle=True)\n",
    "\n",
    "    dataset_dict = {\n",
    "        'train': datasets.Dataset.from_dict({'text' : train, 'midi' : midi_train}),\n",
    "        'test': datasets.Dataset.from_dict({'text' : test,'midi' : midi_test}),\n",
    "        }\n",
    "\n",
    "    dataset = datasets.DatasetDict(dataset_dict)\n",
    "    dataset.save_to_disk('../data/hf_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.PreTrainedTokenizerFast(tokenizer_file='hf_tokenizer')\n",
    "if tokenizer == None:\n",
    "    tokenizer = tokenizers.Tokenizer(tokenizers.models.WordLevel(unk_token=\"[UNK]\"))\n",
    "    trainer = tokenizers.trainers.WordLevelTrainer(special_tokens=[\"[UNK]\",\"<pad>\"])\n",
    "    tokenizer.train_from_iterator(dataset['train']['text'],trainer=trainer)\n",
    "    # [x for x in tokens.tolist() if x not in sorted(list(tokenizer.get_vocab().keys()))]\n",
    "    tokenizer.save('../data/hf_tokenizer')\n",
    "    \n",
    "# tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=195, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t52: AddedToken(\"<end>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t53: AddedToken(\"<start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.enable_padding(pad_id = tokenizer.token_to_id('<pad>'), pad_token='<pad>', length=max_length)\n",
    "# print(tokenizer.token_to_id('<pad>'))\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_str = dataset['train'][0]['text'][:10]\n",
    "# print(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size\n",
    "tokenizer.pad_token = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenized_datasets == None:\n",
    "    def tokenize(element):\n",
    "        outputs = tokenizer.convert_tokens_to_ids(\n",
    "            element[\"text\"],\n",
    "        )\n",
    "        return {\"input_ids\": outputs}\n",
    "\n",
    "\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize, remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "    tokenized_datasets\n",
    "    tokenized_datasets.save_to_disk('hf_dataset_tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    }
   ],
   "source": [
    "# model = transformers.AutoModelForCausalLM.from_config(\n",
    "#     transformers.GPT2Config(\n",
    "#         vocab_size=200, #tokenizer.vocab_size,\n",
    "#         n_positions=max_length,\n",
    "#         n_embd=128,\n",
    "#         n_layer=2,\n",
    "#         n_head=4,\n",
    "#         # bos_token_id=tokenizer.token_to_id('<start>'),\n",
    "#         # eos_token_id=tokenizer.token_to_id('<end>'),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_config(\n",
    "    transformers.MambaConfig(\n",
    "        vocab_size=200,\n",
    "        hidden_size=128,\n",
    "        state_size=4,\n",
    "        num_hidden_layers=8,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        # bos_token_id=tokenizer.token_to_id('<start>'),\n",
    "        # eos_token_id=tokenizer.token_to_id('<end>'),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 0.9M parameters\n"
     ]
    }
   ],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = transformers.TrainingArguments(\n",
    "    output_dir=\"workspace/data/hf_gpt2_chords\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1_000,\n",
    "    logging_steps=100,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=200,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    # fp16=True,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdazzid\u001b[0m (\u001b[33mmusic_gpt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/src/wandb/run-20240326_154929-jbgepc4o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/music_gpt/huggingface/runs/jbgepc4o' target=\"_blank\">rural-resonance-7</a></strong> to <a href='https://wandb.ai/music_gpt/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/music_gpt/huggingface' target=\"_blank\">https://wandb.ai/music_gpt/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/music_gpt/huggingface/runs/jbgepc4o' target=\"_blank\">https://wandb.ai/music_gpt/huggingface/runs/jbgepc4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1711468170.568244] [d72355bbe943:1003952:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='135200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     6/135200 04:30 < 2542:27:22, 0.01 it/s, Epoch 0.01/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
