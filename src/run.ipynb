{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from trainer import Trainer, TrainerConfig\n",
    "from mingpt_utils import set_seed\n",
    "from model import GPT, GPTConfig\n",
    "import torch\n",
    "from utils import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from mingpt_utils import sample\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:  2\n",
      "torch version: 2.2.1+cu121\n",
      "cudnn version: 8902\n",
      "cuda version: 12.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Available devices: \", torch.cuda.device_count())\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cudnn version:\", torch.backends.cudnn.version())\n",
    "print(\"cuda version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 2048\n",
    "id = 0\n",
    "tokens = np.load('../data/formatted/tokens.npy', allow_pickle=True)\n",
    "train = np.load('../data/shuffled/dataset_train.npy', allow_pickle=True)\n",
    "test = np.load('../data/shuffled/dataset_test.npy', allow_pickle=True)\n",
    "midi_train = np.load('../data/shuffled/midi_train.npy', allow_pickle=True)\n",
    "midi_test = np.load('../data/shuffled/midi_test.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert midi into dtype int\n",
    "midi_train = midi_train.astype(int)\n",
    "midi_test = midi_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43272, 2048) (4800, 2048) (43272, 2048, 8) (4800, 2048, 8)\n",
      "data has 43272 pieces, 195 unique tokens.\n",
      "data has 4800 pieces, 195 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape, midi_train.shape, midi_test.shape)\n",
    "\n",
    "dataset = TokenDatasetMidi(train, midi_train,  max_length, tokens)\n",
    "validation = TokenDatasetMidi(test, midi_test, max_length, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 12:58:29 - ERROR - wandb.jupyter -   Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdazzid\u001b[0m (\u001b[33mmusic_gpt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/src/wandb/run-20240312_125831-4ahuuj2u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/music_gpt/music_gpt_new_voicing/runs/4ahuuj2u' target=\"_blank\">iconic-water-8</a></strong> to <a href='https://wandb.ai/music_gpt/music_gpt_new_voicing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/music_gpt/music_gpt_new_voicing' target=\"_blank\">https://wandb.ai/music_gpt/music_gpt_new_voicing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/music_gpt/music_gpt_new_voicing/runs/4ahuuj2u' target=\"_blank\">https://wandb.ai/music_gpt/music_gpt_new_voicing/runs/4ahuuj2u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/music_gpt/music_gpt_new_voicing/runs/4ahuuj2u?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbde3a58af0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "#wandb.login()\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"music_gpt_new_voicing\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"architecture\": \"Transformer - minGPT\",\n",
    "    \"dataset\": \"chords from iRealPro\",\n",
    "    \"epochs\": 250,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 12:58:32 - INFO - model -   number of parameters: 1.861376e+06\n",
      "03/12/2024 12:58:32 - INFO - model -   number of parameters: 1.861376e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/model_epochs->250_heads->4_embd->192_batch->32_new_midi_embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1352: train loss 1.69695. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:18<00:00,  4.24it/s]\n",
      "03/12/2024 13:03:52 - INFO - trainer -   epoch train loss: 2.799219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.799219166095928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 13:04:06 - INFO - trainer -   test loss: 1.646985\n",
      "epoch 2 iter 1352: train loss 1.45854. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:18<00:00,  4.25it/s]\n",
      "03/12/2024 13:09:25 - INFO - trainer -   epoch train loss: 1.584151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.5841505897547348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 13:09:39 - INFO - trainer -   test loss: 1.481287\n",
      "epoch 3 iter 1352: train loss 1.39143. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:18<00:00,  4.25it/s]\n",
      "03/12/2024 13:14:57 - INFO - trainer -   epoch train loss: 1.444626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.444626359435601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 13:15:11 - INFO - trainer -   test loss: 1.427665\n",
      "epoch 4 iter 1352: train loss 1.27422. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:18<00:00,  4.24it/s]\n",
      "03/12/2024 13:20:30 - INFO - trainer -   epoch train loss: 1.338538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.3385379384908513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 13:20:44 - INFO - trainer -   test loss: 1.271491\n",
      "epoch 5 iter 1352: train loss 1.19974. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:18<00:00,  4.25it/s]\n",
      "03/12/2024 13:26:03 - INFO - trainer -   epoch train loss: 1.256074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.2560737678410825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 13:26:17 - INFO - trainer -   test loss: 1.227735\n",
      "epoch 6 iter 1352: train loss 1.17343. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:17<00:00,  4.26it/s]\n",
      "03/12/2024 13:31:35 - INFO - trainer -   epoch train loss: 1.220359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.2203588897179254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 13:31:49 - INFO - trainer -   test loss: 1.205947\n",
      "epoch 7 iter 1352: train loss 1.13213. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:17<00:00,  4.26it/s]\n",
      "03/12/2024 13:37:07 - INFO - trainer -   epoch train loss: 1.195299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.195299013314737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 13:37:21 - INFO - trainer -   test loss: 1.184742\n",
      "epoch 8 iter 1352: train loss 1.12346. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:17<00:00,  4.26it/s]\n",
      "03/12/2024 13:42:39 - INFO - trainer -   epoch train loss: 1.173387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.1733866373169273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 13:42:53 - INFO - trainer -   test loss: 1.161737\n",
      "epoch 9 iter 1352: train loss 1.10216. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:18<00:00,  4.25it/s]\n",
      "03/12/2024 13:48:12 - INFO - trainer -   epoch train loss: 1.153956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.1539562235738468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 13:48:26 - INFO - trainer -   test loss: 1.145953\n",
      "epoch 10 iter 1352: train loss 1.08275. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:18<00:00,  4.25it/s]\n",
      "03/12/2024 13:53:44 - INFO - trainer -   epoch train loss: 1.137876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.1378758466428769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 13:53:58 - INFO - trainer -   test loss: 1.122994\n",
      "epoch 11 iter 1352: train loss 1.06621. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:17<00:00,  4.25it/s]\n",
      "03/12/2024 13:59:17 - INFO - trainer -   epoch train loss: 1.122520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.1225195069358864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 13:59:30 - INFO - trainer -   test loss: 1.100284\n",
      "epoch 12 iter 1352: train loss 1.05141. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:18<00:00,  4.25it/s]\n",
      "03/12/2024 14:04:49 - INFO - trainer -   epoch train loss: 1.105544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.1055436424652207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 14:05:03 - INFO - trainer -   test loss: 1.077704\n",
      "epoch 13 iter 1352: train loss 1.02099. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:17<00:00,  4.26it/s]\n",
      "03/12/2024 14:10:21 - INFO - trainer -   epoch train loss: 1.087544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.0875439282179056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 14:10:35 - INFO - trainer -   test loss: 1.059534\n",
      "epoch 14 iter 1352: train loss 1.00255. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:17<00:00,  4.26it/s]\n",
      "03/12/2024 14:15:53 - INFO - trainer -   epoch train loss: 1.072636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.0726359817863655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 14:16:07 - INFO - trainer -   test loss: 1.049797\n",
      "epoch 15 iter 1352: train loss 0.98366. lr 3.000000e-05: 100%|██████████| 1353/1353 [05:18<00:00,  4.25it/s]\n",
      "03/12/2024 14:21:25 - INFO - trainer -   epoch train loss: 1.060054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.060053651498497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2024 14:21:39 - INFO - trainer -   test loss: 1.036180\n",
      "epoch 16 iter 898: train loss 1.08910. lr 3.000000e-05:  66%|██████▋   | 899/1353 [03:30<01:46,  4.25it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "embedding = 192\n",
    "heads = 4\n",
    "layers = 4\n",
    "batch_size = 32\n",
    "learning_rate = 3e-5\n",
    "num_workers = 4\n",
    "midi_vocab = 128\n",
    "\n",
    "mconf = GPTConfig(len(tokens), dataset.block_size, midi_vocab, n_layer=layers, n_head=heads, n_embd=embedding)\n",
    "session_model = GPT(mconf)\n",
    "MODEL_NAME = \"../models/model_\"+ \"epochs->\" + str(epochs) + \"_heads->\" + str(heads) + \"_embd->\" + str(embedding) + \"_batch->\" + str(batch_size) + \"_new_midi_embeddings\"\n",
    "print(MODEL_NAME)\n",
    "\n",
    "session_model = load_model(MODEL_NAME, session_model)\n",
    "\n",
    "if (session_model == None):\n",
    "    #mconf = GPTConfig(len(tokens), dataset.block_size, n_layer=layers, n_head=heads, n_embd=embbedings)\n",
    "    session_model = GPT(mconf)\n",
    "    tconf = TrainerConfig(max_epochs=epochs, \n",
    "                          batch_size=batch_size, \n",
    "                          learning_rate=learning_rate, \n",
    "                          num_workers=num_workers\n",
    "                          )\n",
    "    writer = SummaryWriter(log_dir='../runs/'+'logs') \n",
    "    trainer = Trainer(session_model, dataset, validation, tconf, writer)\n",
    "    trainer.train()\n",
    "    save_model(MODEL_NAME, session_model)\n",
    "    # [optional] finish the wandb run, necessary in notebooks\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
