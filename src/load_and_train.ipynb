{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48072, 2048) (48072, 2048, 8) (48072,)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load(\"../data/formatted/dataset.npy\", allow_pickle=True)\n",
    "midi_dataset = np.load(\"../data/formatted/midi_dataset.npy\", allow_pickle=True)\n",
    "meta_dataset = np.load(\"../data/formatted/meta_augmented.npy\", allow_pickle=True)\n",
    "\n",
    "print(dataset.shape, midi_dataset.shape, meta_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Medium Swing' 'Tonality' 'C major' ... '<pad>' '<pad>' '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "{'A minor', '1.5989583333333333', 'Rock Waltz', 'D major', 'Beatles', 'Form_C', 'Gb major', '0.7994791666666666', 'Power Ballad', 'Db minor', 'add #7', 'E##', '2.0', 'Medium Funk', 'Slow Bossa', '0.5703125', 'C##', 'C major', 'E minor', 'maj6', '0.3997395833333333', 'Fast Swing', 'o_maj7', 'alter #5', '1.1419270833333333', 'Pop', '/', 'Blues', 'A major', 'Electro Pop', 'Funk Rock', 'Fusion', 'March', 'Foxtrot', 'F#', ':|', 'D#', 'G minor', 'Afro-Samba', 'aug', 'Fast Blues', 'E', 'F minor', 'E#', 'Medium Pop', 'Medium Swing', 'alter b5', 'Pop Shuffle', 'C', 'Rock Slow', 'add #11', 'Even 16ths', 'add b13', '0.5', 'Medium Rock', 'Shuffle Blues', 'o7', 'add b9', 'Medium Up', 'Pop-Shuffle', 'Waltz', 'Bolero', 'Samba-Funk', 'G##', 'Repeat_1', 'Blues Rock', 'Blues Shuffle', 'Bossa Acoustic', 'Shuffle', 'Bbb', 'Repeat_2', 'Reggae', 'Eb major', '0.8880208333333334', 'Ab major', 'C#', 'Maxixe', 'RnB', 'Slowly', 'major-13th', 'Medium Slow', 'Medium Up Swing', 'maj7', '2.6666666666666665', '<end>', 'Bolero-Cha', 'Country Ballad', 'm7', 'Ab minor', 'Tonality', 'Traditional Gospel', 'Gypsy Bossa', 'Samba-Rock', 'Jazz', 'Gb minor', 'Med Up Latin', 'Montuno', 'Moderately', 'Choro - Samba', \"Even 8's\", 'Choro', 'add 2', 'Bb', 'Gospel Ballad', 'Rock/Reggae', 'Gary Aprile', 'Dbb', 'power', 'Baião', 'Folk Ballad', 'Pop jazz', '<pad>', '2.3997395833333335', 'Funk', '.', 'Blues Pop', 'Form_verse', 'G major', 'Latin-Swing', 'D##', 'Slow Shuffle', 'Bb minor', 'F major', '|', \"Even 8th's\", 'alter #11', 'alter b9', 'B major', 'Abb', 'Musical', 'Afoxe', 'Rock/Hip Hop', 'add 7', '3.0', 'Form_intro', 'Rock Pop', 'Bright Shuffle Blues', 'Medium Ballad', 'Moderate Latin', 'A', 'Up Swing', 'Cuban Son', 'Frevo', 'Cha Cha', 'A#', 'Gb', 'Db', 'Worship', 'Merengue', 'B#', 'm', 'Reggae Pop', 'Deliberately', 'dom7', 'Samba Funk', 'Forró', 'Fb', 'Son', 'C minor', 'Calypso', 'Hymn', '<start>', 'Eb minor', 'Folk', 'alter #9', 'Bb major', 'B##', 'add 13', 'Funk Jazz', '128 Feel', 'Samba', 'add 11', 'Form_Segno', 'Ebb', 'Gypsy Swing', 'Medium Shuffle', 'Slow Ballad', 'Salsa', 'D minor', 'Gypsy Waltz', 'Bolero-Son', '0.6666666666666666', '4.0', 'A##', 'Even 8ths', 'Dreamlike', 'Folk Rock', 'Form_Coda', 'Rock-Folk', 'Chacarera', 'Pop Rock', 'Rock', 'Cbb', 'sus4', '1.7135416666666667', 'add #5', 'Up Samba', 'Ballad', 'Electric Blues', '1.0', 'Ab', 'Fbb', \"Rock'n'Roll\", 'Soul Ballad', 'Marchinha', 'Repeat_0', 'Afoxé', 'Rock Ballad', '1.3333333333333333', 'F##', 'Afro', 'Folk-Rock', 'Mambo', '0.75', 'Slow Swing', 'Pop Ballad', 'Disco', 'add 9', 'o', 'Form_B', 'Gbb', 'E major', 'add b6', 'Samba Reggae', 'Bossa Nova', '1.5', 'D', 'Soul', 'F', 'Slow Blues', 'B', 'Medium Country', 'Disco Funk', 'add #9', \"R'n'B\", 'Up Tempo', 'G', 'Form_A', 'Up Tempo Swing', 'sus7', 'Tango', 'Pop Folk', 'Afoxé-Samba', '2.25', 'Up Waltz (One Feel)', 'N.C.', 'Slow Rock', 'Cb', 'Samba Enredo', 'Form_D', 'G#', 'Repeat_3', 'Latin', 'm6', 'Medium Waltz', '0.4440104166666667', 'Rock Blues', 'Eb', 'B minor', 'Country Blues', 'maj', 'Medium Blues', 'Db major', 'UP Swing'}\n"
     ]
    }
   ],
   "source": [
    "#Token from dataset\n",
    "\n",
    "chords_array = [[item for item in row] for row in dataset]\n",
    "concatenated_array = np.hstack(chords_array) \n",
    "tokens = set(concatenated_array)\n",
    "\n",
    "print(len(tokens))\n",
    "print(tokens)\n",
    "np.save(\"../data/formatted/tokens.npy\", list(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A minor': 0, '1.5989583333333333': 1, 'Rock Waltz': 2, 'D major': 3, 'Beatles': 4, 'Form_C': 5, 'Gb major': 6, '0.7994791666666666': 7, 'Power Ballad': 8, 'Db minor': 9, 'add #7': 10, 'E##': 11, '2.0': 12, 'Medium Funk': 13, 'Slow Bossa': 14, '0.5703125': 15, 'C##': 16, 'C major': 17, 'E minor': 18, 'maj6': 19, '0.3997395833333333': 20, 'Fast Swing': 21, 'o_maj7': 22, 'alter #5': 23, '1.1419270833333333': 24, 'Pop': 25, '/': 26, 'Blues': 27, 'A major': 28, 'Electro Pop': 29, 'Funk Rock': 30, 'Fusion': 31, 'March': 32, 'Foxtrot': 33, 'F#': 34, ':|': 35, 'D#': 36, 'G minor': 37, 'Afro-Samba': 38, 'aug': 39, 'Fast Blues': 40, 'E': 41, 'F minor': 42, 'E#': 43, 'Medium Pop': 44, 'Medium Swing': 45, 'alter b5': 46, 'Pop Shuffle': 47, 'C': 48, 'Rock Slow': 49, 'add #11': 50, 'Even 16ths': 51, 'add b13': 52, '0.5': 53, 'Medium Rock': 54, 'Shuffle Blues': 55, 'o7': 56, 'add b9': 57, 'Medium Up': 58, 'Pop-Shuffle': 59, 'Waltz': 60, 'Bolero': 61, 'Samba-Funk': 62, 'G##': 63, 'Repeat_1': 64, 'Blues Rock': 65, 'Blues Shuffle': 66, 'Bossa Acoustic': 67, 'Shuffle': 68, 'Bbb': 69, 'Repeat_2': 70, 'Reggae': 71, 'Eb major': 72, '0.8880208333333334': 73, 'Ab major': 74, 'C#': 75, 'Maxixe': 76, 'RnB': 77, 'Slowly': 78, 'major-13th': 79, 'Medium Slow': 80, 'Medium Up Swing': 81, 'maj7': 82, '2.6666666666666665': 83, '<end>': 84, 'Bolero-Cha': 85, 'Country Ballad': 86, 'm7': 87, 'Ab minor': 88, 'Tonality': 89, 'Traditional Gospel': 90, 'Gypsy Bossa': 91, 'Samba-Rock': 92, 'Jazz': 93, 'Gb minor': 94, 'Med Up Latin': 95, 'Montuno': 96, 'Moderately': 97, 'Choro - Samba': 98, \"Even 8's\": 99, 'Choro': 100, 'add 2': 101, 'Bb': 102, 'Gospel Ballad': 103, 'Rock/Reggae': 104, 'Gary Aprile': 105, 'Dbb': 106, 'power': 107, 'Baião': 108, 'Folk Ballad': 109, 'Pop jazz': 110, '<pad>': 111, '2.3997395833333335': 112, 'Funk': 113, '.': 114, 'Blues Pop': 115, 'Form_verse': 116, 'G major': 117, 'Latin-Swing': 118, 'D##': 119, 'Slow Shuffle': 120, 'Bb minor': 121, 'F major': 122, '|': 123, \"Even 8th's\": 124, 'alter #11': 125, 'alter b9': 126, 'B major': 127, 'Abb': 128, 'Musical': 129, 'Afoxe': 130, 'Rock/Hip Hop': 131, 'add 7': 132, '3.0': 133, 'Form_intro': 134, 'Rock Pop': 135, 'Bright Shuffle Blues': 136, 'Medium Ballad': 137, 'Moderate Latin': 138, 'A': 139, 'Up Swing': 140, 'Cuban Son': 141, 'Frevo': 142, 'Cha Cha': 143, 'A#': 144, 'Gb': 145, 'Db': 146, 'Worship': 147, 'Merengue': 148, 'B#': 149, 'm': 150, 'Reggae Pop': 151, 'Deliberately': 152, 'dom7': 153, 'Samba Funk': 154, 'Forró': 155, 'Fb': 156, 'Son': 157, 'C minor': 158, 'Calypso': 159, 'Hymn': 160, '<start>': 161, 'Eb minor': 162, 'Folk': 163, 'alter #9': 164, 'Bb major': 165, 'B##': 166, 'add 13': 167, 'Funk Jazz': 168, '128 Feel': 169, 'Samba': 170, 'add 11': 171, 'Form_Segno': 172, 'Ebb': 173, 'Gypsy Swing': 174, 'Medium Shuffle': 175, 'Slow Ballad': 176, 'Salsa': 177, 'D minor': 178, 'Gypsy Waltz': 179, 'Bolero-Son': 180, '0.6666666666666666': 181, '4.0': 182, 'A##': 183, 'Even 8ths': 184, 'Dreamlike': 185, 'Folk Rock': 186, 'Form_Coda': 187, 'Rock-Folk': 188, 'Chacarera': 189, 'Pop Rock': 190, 'Rock': 191, 'Cbb': 192, 'sus4': 193, '1.7135416666666667': 194, 'add #5': 195, 'Up Samba': 196, 'Ballad': 197, 'Electric Blues': 198, '1.0': 199, 'Ab': 200, 'Fbb': 201, \"Rock'n'Roll\": 202, 'Soul Ballad': 203, 'Marchinha': 204, 'Repeat_0': 205, 'Afoxé': 206, 'Rock Ballad': 207, '1.3333333333333333': 208, 'F##': 209, 'Afro': 210, 'Folk-Rock': 211, 'Mambo': 212, '0.75': 213, 'Slow Swing': 214, 'Pop Ballad': 215, 'Disco': 216, 'add 9': 217, 'o': 218, 'Form_B': 219, 'Gbb': 220, 'E major': 221, 'add b6': 222, 'Samba Reggae': 223, 'Bossa Nova': 224, '1.5': 225, 'D': 226, 'Soul': 227, 'F': 228, 'Slow Blues': 229, 'B': 230, 'Medium Country': 231, 'Disco Funk': 232, 'add #9': 233, \"R'n'B\": 234, 'Up Tempo': 235, 'G': 236, 'Form_A': 237, 'Up Tempo Swing': 238, 'sus7': 239, 'Tango': 240, 'Pop Folk': 241, 'Afoxé-Samba': 242, '2.25': 243, 'Up Waltz (One Feel)': 244, 'N.C.': 245, 'Slow Rock': 246, 'Cb': 247, 'Samba Enredo': 248, 'Form_D': 249, 'G#': 250, 'Repeat_3': 251, 'Latin': 252, 'm6': 253, 'Medium Waltz': 254, '0.4440104166666667': 255, 'Rock Blues': 256, 'Eb': 257, 'B minor': 258, 'Country Blues': 259, 'maj': 260, 'Medium Blues': 261, 'Db major': 262, 'UP Swing': 263}\n"
     ]
    }
   ],
   "source": [
    "stoi = { tk:i for i,tk in enumerate(tokens) }\n",
    "itos = { i:tk for i,tk in enumerate(tokens) }\n",
    "\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Get the number of real songs\n",
    "realSongs = len(dataset)/12\n",
    "tenPercent = int(0.1 * realSongs)\n",
    "\n",
    "#random a number without repeating number\n",
    "randomList = random.sample(range(0, int(realSongs)), tenPercent)\n",
    "\n",
    "#if number is bigger than 12 multiply it by 12\n",
    "for i in range(len(randomList)):\n",
    "    randomList[i] = randomList[i] * 12\n",
    "\n",
    "#populate a random list with the 12 subsequent numbers per value\n",
    "final_random_list=[]\n",
    "for number in randomList:\n",
    "    for i in range(12):\n",
    "        final_random_list.append(number+i)\n",
    "\n",
    "#check if a number is duplicated \n",
    "print(len(final_random_list) == len(set(final_random_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the random list\n",
    "np.save('../data/formatted/final_random_list.npy', final_random_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset and validation using the random list\n",
    "dataset_test = dataset[final_random_list]\n",
    "midi_test = midi_dataset[final_random_list]\n",
    "meta_test = meta_dataset[final_random_list]\n",
    "\n",
    "dataset_train = np.delete(dataset, final_random_list, axis=0)\n",
    "midi_train = np.delete(midi_dataset, final_random_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the train and test dataset\n",
    "#test\n",
    "np.save('../data/formatted/dataset_test.npy', dataset_test)\n",
    "np.save('../data/formatted/midi_test.npy', midi_test)\n",
    "np.save('../data/formatted/meta_test.npy', meta_test)\n",
    "#train\n",
    "np.save('../data/formatted/dataset_train.npy', dataset_train)\n",
    "np.save('../data/formatted/midi_train.npy', midi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#create a file with shuffled reference index\n",
    "def createWindowedShuffleReference(type, size, window, save = False):\n",
    "    s = np.arange(0, size, 1)\n",
    "    #num = np.arange(0, len(data)/10, 1)\n",
    "    np.random.shuffle(s)\n",
    "\n",
    "    n = int(size/window)\n",
    "    numlist = random.sample(range(n), n)\n",
    "    numlist = np.array(numlist)\n",
    "    numlist = numlist * window\n",
    "\n",
    "    m = np.max(numlist)\n",
    "    l_ref = size-window\n",
    "    print('real:', size, 'max:', m, 'length_ref:',l_ref)\n",
    "\n",
    "    if m != l_ref:\n",
    "        rest = m - l_ref\n",
    "        numlist = numlist - rest\n",
    "\n",
    "    ref = []\n",
    "    for num in numlist:\n",
    "        if num == 0:\n",
    "            print(\"OK\")\n",
    "        for i in range(0,window):\n",
    "            ref.append(num+i)\n",
    "\n",
    "    #return the shuffled list\n",
    "    if save:\n",
    "        np.savetxt(\"../data/shuffle_\" + type + \".txt\", ref, fmt='%i', delimiter=\" \", header='Array shape: ('+str(size)+', 1)')\n",
    "    return ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(folder, name):\n",
    "    data_path = folder + '/' + name\n",
    "    data = np.loadtxt(data_path)\n",
    "    f = open(data_path, \"r\")\n",
    "    format = f.readline().replace('# Array shape: (', '').replace('\\n', '').replace(')', '')\n",
    "    format = np.array(format.split(', ')).astype(int)\n",
    "    f.close()\n",
    "    return data, format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43272, 2048) (43272, 2048, 8) (4800, 2048) (4800, 2048, 8)\n",
      "real: 43272 max: 43271 length_ref: 43271\n",
      "OK\n",
      "real: 4800 max: 4799 length_ref: 4799\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "train_dataset = np.load('../data/formatted/dataset_train.npy', allow_pickle=True)\n",
    "test_dataset = np.load('../data/formatted/dataset_test.npy', allow_pickle=True)\n",
    "\n",
    "train_midi = np.load('../data/formatted/midi_train.npy', allow_pickle=True)\n",
    "test_midi = np.load('../data/formatted/midi_test.npy', allow_pickle=True)\n",
    "\n",
    "print(train_dataset.shape, train_midi.shape, test_dataset.shape, test_midi.shape)\n",
    "\n",
    "BATCH_SHUFFLE_SIZE = 1\n",
    "ref = createWindowedShuffleReference(\"train\", len(train_dataset), BATCH_SHUFFLE_SIZE, True)\n",
    "ref_test = createWindowedShuffleReference(\"test\", len(test_dataset), BATCH_SHUFFLE_SIZE, True)\n",
    "\n",
    "# first shuffle the train dataset\n",
    "shuffle_train, format_train = getData('../data', 'shuffle_train.txt')\n",
    "shuffle_train = shuffle_train.reshape(format_train[0], ).astype(int)\n",
    "shuffle_train = shuffle_train.tolist()\n",
    "dataset = train_dataset[shuffle_train]\n",
    "midiDataset = train_midi[shuffle_train]\n",
    "\n",
    "#second shuffle the test dataset\n",
    "shuffle_test, format_test = getData('../data', 'shuffle_test.txt')\n",
    "shuffle_test = shuffle_test.reshape(format_test[0], ).astype(int)\n",
    "shuffle_test = shuffle_test.tolist()\n",
    "validation = test_dataset[shuffle_test]\n",
    "midi_validation = test_midi[shuffle_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/shuffled/dataset_train.npy', dataset)\n",
    "np.save('../data/shuffled/midi_train.npy', midiDataset)\n",
    "np.save('../data/shuffled/dataset_test.npy', validation)\n",
    "np.save('../data/shuffled/midi_test.npy', midi_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import *\n",
    "\n",
    "max_length = 2048\n",
    "id = 0\n",
    "tokens = np.load('../data/formatted/tokens.npy', allow_pickle=True)\n",
    "train = np.load('../data/shuffled/dataset_train.npy', allow_pickle=True)\n",
    "test = np.load('../data/shuffled/dataset_test.npy', allow_pickle=True)\n",
    "midi_train = np.load('../data/shuffled/midi_train.npy', allow_pickle=True)\n",
    "midi_test = np.load('../data/shuffled/midi_test.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43272, 2048) (4800, 2048) (43272, 2048, 8) (4800, 2048, 8)\n",
      "data has 43272 pieces, 264 unique tokens.\n",
      "data has 4800 pieces, 264 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape, midi_train.shape, midi_test.shape)\n",
    "\n",
    "dataset = TokenDatasetMidi(train, midi_train,  max_length, tokens)\n",
    "validation = TokenDatasetMidi(test, midi_test, max_length, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/11/2024 19:13:25 - ERROR - wandb.jupyter -   Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdazzid\u001b[0m (\u001b[33mmusic_gpt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/src/wandb/run-20240311_191326-ax5y2ive</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/music_gpt/music_gpt_new_voicing/runs/ax5y2ive' target=\"_blank\">leafy-feather-4</a></strong> to <a href='https://wandb.ai/music_gpt/music_gpt_new_voicing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/music_gpt/music_gpt_new_voicing' target=\"_blank\">https://wandb.ai/music_gpt/music_gpt_new_voicing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/music_gpt/music_gpt_new_voicing/runs/ax5y2ive' target=\"_blank\">https://wandb.ai/music_gpt/music_gpt_new_voicing/runs/ax5y2ive</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/music_gpt/music_gpt_new_voicing/runs/ax5y2ive?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f6a4fdd7940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "#wandb.login()\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"music_gpt_new_voicing\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"architecture\": \"Transformer - minGPT\",\n",
    "    \"dataset\": \"chords from iRealPro\",\n",
    "    \"epochs\": 250,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:  2\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from trainer import Trainer, TrainerConfig\n",
    "from mingpt_utils import set_seed\n",
    "from model import GPT, GPTConfig\n",
    "import torch\n",
    "print(\"Available devices: \", torch.cuda.device_count())\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from mingpt_utils import sample\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:  2\n",
      "torch version: 2.2.1+cu121\n",
      "cudnn version: 8902\n",
      "cuda version: 12.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Available devices: \", torch.cuda.device_count())\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cudnn version:\", torch.backends.cudnn.version())\n",
    "print(\"cuda version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/11/2024 19:14:03 - INFO - model -   number of parameters: 1.887872e+06\n",
      "03/11/2024 19:14:03 - INFO - model -   number of parameters: 1.887872e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/model_epochs->250_heads->4_embd->192_batch->64_new_midi_embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/677 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/workspace/src/utils.py\", line 101, in __getitem__\n    dix = [self.stoi[s] for s in chunk[0]]\n  File \"/workspace/src/utils.py\", line 101, in <listcomp>\n    dix = [self.stoi[s] for s in chunk[0]]\nKeyError: 4.0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../runs/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(session_model, dataset, validation, tconf, writer)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m save_model(MODEL_NAME, session_model)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# [optional] finish the wandb run, necessary in notebooks\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/src/trainer.py:181\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# counter used for learning rate decay\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m         test_loss \u001b[38;5;241m=\u001b[39m run_epoch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/workspace/src/trainer.py:76\u001b[0m, in \u001b[0;36mTrainer.train.<locals>.run_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     74\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     75\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader)) \u001b[38;5;28;01mif\u001b[39;00m is_train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it, (x, y, m) \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m#print('x:', x.shape, 'y:', y.shape, 'm:',  m.shape)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# place data on the correct device\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     81\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/workspace/src/utils.py\", line 101, in __getitem__\n    dix = [self.stoi[s] for s in chunk[0]]\n  File \"/workspace/src/utils.py\", line 101, in <listcomp>\n    dix = [self.stoi[s] for s in chunk[0]]\nKeyError: 4.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "embedding = 192\n",
    "heads = 4\n",
    "layers = 4\n",
    "batch_size = 64\n",
    "learning_rate = 3e-5\n",
    "num_workers = 4\n",
    "midi_vocab = 128\n",
    "\n",
    "mconf = GPTConfig(len(tokens), dataset.block_size, midi_vocab, n_layer=layers, n_head=heads, n_embd=embedding)\n",
    "session_model = GPT(mconf)\n",
    "MODEL_NAME = \"../models/model_\"+ \"epochs->\" + str(epochs) + \"_heads->\" + str(heads) + \"_embd->\" + str(embedding) + \"_batch->\" + str(batch_size) + \"_new_midi_embeddings\"\n",
    "print(MODEL_NAME)\n",
    "\n",
    "session_model = load_model(MODEL_NAME, session_model)\n",
    "\n",
    "if (session_model == None):\n",
    "    #mconf = GPTConfig(len(tokens), dataset.block_size, n_layer=layers, n_head=heads, n_embd=embbedings)\n",
    "    session_model = GPT(mconf)\n",
    "    tconf = TrainerConfig(max_epochs=epochs, \n",
    "                          batch_size=batch_size, \n",
    "                          learning_rate=learning_rate, \n",
    "                          num_workers=num_workers\n",
    "                          )\n",
    "    writer = SummaryWriter(log_dir='../runs/'+'logs') \n",
    "    trainer = Trainer(session_model, dataset, validation, tconf, writer)\n",
    "    trainer.train()\n",
    "    save_model(MODEL_NAME, session_model)\n",
    "    # [optional] finish the wandb run, necessary in notebooks\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
